leader_2_link <- sapply(country_nodes, function(x) {html_nodes(x, xpath = "./ul/li[2]/a[1]") %>% html_attr("href")})
leader_2_link[lengths(leader_2_link) == 0] <- ""
# compile data frame
dat <- data.frame(country = country_text,
leader_1 = unlist(leader_1_text),
leader_1_link = unlist(leader_1_link),
leader_2 = unlist(leader_2_text),
leader_2_link = unlist(leader_2_link),
stringsAsFactors = FALSE)
View(dat)
# load RSelenium
library(tidyverse)
library(rvest)
library(RSelenium)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
# close connection
remDr$closeServer()
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# load RSelenium
library(tidyverse)
library(rvest)
library(RSelenium)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# enter keyword in title field
xpath <- '//*[@id="main"]/div[1]/div[2]/input'
titleElem <- remDr$findElement(using = 'xpath', value = xpath)
titleElem$sendKeysToElement(list("data")) # enter key word
# select requested title data
xpath <- '//*[@id="main"]/div[8]/div[2]/select/option[7]' # plot
titledatElem1 <- remDr$findElement(using = 'xpath', value = xpath)
titledatElem1$clickElement() # click on list element
xpath <- '//*[@id="main"]/div[8]/div[2]/select/option[10]' # technical info
titledatElem2 <- remDr$findElement(using = 'xpath', value = xpath)
titledatElem2$clickElement() # click on list element
# scroll to end of page (just for fun)
webElem <- remDr$findElement("css", "body")
webElem$sendKeysToElement(list(key = "end"))
# click on search button
xpath <- '//*[@id="main"]/p[3]/button'
searchElem <- remDr$findElement(using = 'xpath', value = xpath)
searchElem$clickElement() # click on button
# store index page
output <- remDr$getPageSource(header = TRUE)
write(output[[1]], file = "data/imdb-data-movies.html")
# close connection
remDr$closeServer()
# parse html
content <- read_html("data/imdb-data-movies.html")
titles <- html_nodes(content, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "lister-item-header", " " ))]//a') %>% html_text
head(titles)
write(output[[1]], file = "imdb-data-movies.html")
dir.create("data")
write(output[[1]], file = "data/imdb-data-movies.html")
# close connection
remDr$closeServer()
# parse html
content <- read_html("data/imdb-data-movies.html")
titles <- html_nodes(content, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "lister-item-header", " " ))]//a') %>% html_text
head(titles)
I tried different things with importing them with pdf_text or different OCR packages but I quite never found efficient ways to then import and clean data in bulk.
Thanks!
<br>
library(tidyverse)
library(rvest)
library(pdftools)
# devtools::install_github("hrbrmstr/nominatim")
library(nominatim)
## step 1: inspect page
url <- "http://ajps.org/list-of-reviewers/"
## step 3: import pdf
rev_raw <- pdftools::pdf_text("ajps-reviewers/reviewers2015.pdf")
class(rev_raw)
rev_raw[1]
## step 4: tidy data
rev_all <- rev_raw %>% str_split("\\n") %>% unlist
surname <- str_extract(rev_all, "[[:alpha:]-]+")
prename <- str_extract(rev_all, " [.[:alpha:]]+")
rev_df <- data.frame(raw = rev_all, surname = surname, prename = prename, stringsAsFactors = F)
rev_df$institution <- NA
for(i in 1:nrow(rev_df)) {
rev_df$institution[i] <- rev_df$raw[i] %>% str_replace(rev_df$surname[i], "") %>% str_replace(rev_df$prename[i], "") %>% str_trim()
}
rev_df <- rev_df[-c(1,2),]
rev_df <- rev_df[!is.na(rev_df$surname),]
head(rev_df)
library(tidyverse)
library(rvest)
library(stringr)
# devtools::install_github("hrbrmstr/nominatim")
library(nominatim)
library(sf)
library(rnaturalearth)
browseURL("http://www.biermap24.de/brauereiliste.php")
## step 1: fetch list of cities with breweries
url <- "http://www.biermap24.de/brauereiliste.php"
content <- read_html(url)
class(content)
anchors <- html_nodes(content, xpath = "//tr/td[3]")
cities <- html_text(anchors)
head(cities)
cities <- str_trim(cities)
length(cities)
length(unique(cities))
sort(table(cities))
unique_cities <- unique(cities)
## step 2: geocode cities
# get free key for mapquest API at browseURL("https://developer.mapquest.com/")
load("/Users/s.munzert/rkeys.RDa") # import API key (or paste it here in openstreetmap object)
nominatim::osm_search("Berlin")
nominatim::osm_search("Berlin", key = openstreetmap)
dir()
n_cities <- length(unique_cities)
coords_df <- data.frame(city = rep(NA, n_cities), lon = rep(NA, n_cities), lat = rep(NA, n_cities), stringsAsFactors = FALSE)
if (!file.exists("coords_breweries.RData")){
for (i in 1:n_cities) {
coords <- try(nominatim::osm_search(unique_cities[i], country_codes = "de", key = openstreetmap))
coords_df$city[i] <- unique_cities[i]
if(nrow(coords) > 0) {
coords_df$lon[i] <- as.numeric(coords$lon)
coords_df$lat[i] <- as.numeric(coords$lat)
}else{
coords_df$lon[i] <- NA
coords_df$lat[i] <- NA
}
}
save(coords_df, file="coords_breweries.RData")
} else {
load("coords_breweries.RData")
}
head(coords_df)
## step 3: plot breweries of Germany
pos <- filter(coords_df, lon >= 6, lon <= 15, lat >= 47, lat <= 55)
worldmap <- rnaturalearth::ne_countries(scale = 'medium', type = 'map_units', returnclass = 'sf')
germany <- worldmap[worldmap$name == 'Germany',]
ggplot() + geom_sf(data = germany) + theme_bw() + geom_point(aes(lon,lat), data = pos, size = .5, color = "red")
cities
?read_html
content <- read_html(url, encoding = "latin1")
content
anchors <- html_nodes(content, xpath = "//tr/td[3]")
cities <- html_text(anchors)
cities
content <- read_html(url, encoding = "latin1")
anchors <- html_nodes(content, xpath = "//tr/td[3]")
cities <- html_text(anchors)
cities
library(tidyverse)
library(rvest)
library(pdftools)
# devtools::install_github("hrbrmstr/nominatim")
library(nominatim)
## step 1: inspect page
url <- "http://ajps.org/list-of-reviewers/"
browseURL(url)
## step 3: import pdf
rev_raw <- pdftools::pdf_text("ajps-reviewers/reviewers2015.pdf")
class(rev_raw)
rev_raw[1]
## step 4: tidy data
rev_all <- rev_raw %>% str_split("\\n") %>% unlist
rev_all
surname <- str_extract(rev_all, "[[:alpha:]-]+")
prename <- str_extract(rev_all, " [.[:alpha:]-]+")
## step 4: tidy data
rev_all <- rev_raw %>% str_split("\\n") %>% unlist
surname <- str_extract(rev_all, "[[:alpha:]-]+")
prename <- str_extract(rev_all, " [.[:alpha:]-]+")
rev_df <- data.frame(raw = rev_all, surname = surname, prename = prename, stringsAsFactors = F)
head(rev_df)
rev_df$institution <- NA
rev_df$institution <- NA
for(i in 1:nrow(rev_df)) {
rev_df$institution[i] <- rev_df$raw[i] %>% str_replace(rev_df$surname[i], "") %>% str_replace(rev_df$prename[i], "") %>% str_trim()
}
rev_df <- rev_df[-c(1,2),]
rev_df <- rev_df[!is.na(rev_df$surname),]
head(rev_df)
nrow(rev_df)
?pdf_text
?data.frame
default.stringsAsFactors()
sessionInfo()
?httr
url <- "https://en.wikipedia.org/wiki/List_of_female_scientists_in_the_20th_century"
browseURL(url)
url <- "https://en.wikipedia.org/wiki/List_of_female_scientists_in_the_20th_century"
url_parsed <- read_html(url)
xpath <- "//h2[./span[@id='Anthropology']]/following-sibling::ul[1]/li"
html_nodes(url_parsed, xpath = xpath) %>% html_text()
# load RSelenium
library(tidyverse)
library(rvest)
library(RSelenium)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# enter keyword in title field
xpath <- '//*[@id="main"]/div[1]/div[2]/input'
titleElem <- remDr$findElement(using = 'xpath', value = xpath)
titleElem$sendKeysToElement(list("data")) # enter key word
# select requested title data
xpath <- '//*[@id="main"]/div[8]/div[2]/select/option[7]' # plot
titledatElem1 <- remDr$findElement(using = 'xpath', value = xpath)
titledatElem1$clickElement() # click on list element
xpath <- '//*[@id="main"]/div[8]/div[2]/select/option[10]' # technical info
titledatElem2 <- remDr$findElement(using = 'xpath', value = xpath)
titledatElem2$clickElement() # click on list element
# scroll to end of page (just for fun)
webElem <- remDr$findElement("css", "body")
webElem$sendKeysToElement(list(key = "end"))
# click on search button
xpath <- '//*[@id="main"]/p[3]/button'
searchElem <- remDr$findElement(using = 'xpath', value = xpath)
searchElem$clickElement() # click on button
# store index page
output <- remDr$getPageSource(header = TRUE)
dir.create("data")
output[[1]]
write(output[[1]], file = "data/imdb-data-movies.html")
# close connection
remDr$closeServer()
# parse html
content <- read_html("data/imdb-data-movies.html")
titles <- html_nodes(content, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "lister-item-header", " " ))]//a') %>% html_text
head(titles)
foo <- jsonlite::fromJSON("https://projects.fivethirtyeight.com/2020-election-forecast/house_seats.json")
foo
library(tidyverse)
library(rvest)
library(magrittr)
library(jsonlite)
## overview
browseURL("http://ropensci.org/")
library(ipapi)
devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
library(ipapi)
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
View(ip_df)
# API documentation
browseURL("http://ip-api.com/docs/")
# manual API call, XML data
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::read_xml(url)
ip_list <- as_list(ip_parsed)
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
# manual API call, JSON data
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
as.data.frame(ip_parsed, stringsAsFactors = FALSE)
ip_parsed <- jsonlite::fromJSON(url, flatten = TRUE)
ip_parsed %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
# modify call
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/www.spiegel.de") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
# build function
ipapi_grabber <- function(ip = "") {
dat <- fromJSON(paste0("http://ip-api.com/json/", ip)) %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("193.17.243.1")
#!/usr/local/bin/Rscript
library(stringr)
library(magrittr)
library(httr)
setwd("/Users/simonmunzert/GitHub/ds-workshop-webscraping")
setwd("/Users/s.munzert/GitHub/ds-workshop-webscraping")
setwd("/Users/s.munzert/GitHub/ds-workshop-webscraping/code")
url <- "http://www.spiegel.de/schlagzeilen/"
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
content(url_out, as = "text") %>% write(file = str_c("spiegel-headlines/spiegel-", datetime, ".html"))
#!/usr/local/bin/Rscript
library(stringr)
library(magrittr)
library(httr)
setwd("/Users/s.munzert/GitHubds-workshop-webscraping/code")
setwd("/Users/s.munzert/GitHub/ds-workshop-webscraping/code")
url <- "http://www.spiegel.de/schlagzeilen/"
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
content(url_out, as = "text") %>% write(file = str_c("spiegel-headlines/spiegel-", datetime, ".html"))
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
library(ipapi)
## overview
browseURL("http://ropensci.org/")
browseURL("https://cran.r-project.org/web/views/WebTechnologies.html")
# API documentation
browseURL("http://ip-api.com/")
library(ipapi)
?geolocate
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
# API documentation
browseURL("http://ip-api.com/docs/")
# manual API call, XML data
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::read_xml(url)
ip_parsed
ip_list <- as_list(ip_parsed)
ip_list
ip_list %>% unlist
ip_list %>% unlist %>% t
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
# manual API call, XML data
url <- "http://ip-api.com/xml/72.33.67.89"
ip_parsed <- xml2::read_xml(url)
ip_list <- as_list(ip_parsed)
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
# manual API call, JSON data
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed
as.data.frame(ip_parsed, stringsAsFactors = FALSE)
ip_parsed <- jsonlite::fromJSON(url, flatten = TRUE)
ip_parsed
# modify call
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/www.spiegel.de") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
# build function
ipapi_grabber <- function(ip = "") {
dat <- fromJSON(paste0("http://ip-api.com/json/", ip)) %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("193.17.243.1")
?t
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t
library(rtweet)
install.packages("rtweet")
library(rvest)
library(httr)
?html_read
?read_html
# add User-Agent string
url <- "http://spiegel.de/schlagzeilen"
browseURL("http://www.whoishostingthis.com/tools/user-agent/")
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36"
session <- html_session(url, user_agent(uastring))
# add header fields with rvest + httr
session <- html_session(url, add_headers(From = "my@email.com", `User-Agent` = "R Scraper"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
# add header fields with httr::GET
browseURL("http://httpbin.org")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
GET("http://httpbin.org/headers")
R.Version()$version.string
GET("http://httpbin.org/headers", add_headers(`User-Agent` = "Hello there!"))
GET("http://httpbin.org/headers", add_headers(`User-Agent` = R.Version()$version.string))
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
# example
url_response <- GET("http://spiegel.de/schlagzeilen",
add_headers(From = "my@email.com"))
url_parsed <- url_response  %>% read_html()
url_parsed %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
url_parsed %>% html_nodes(".mr-6") %>%  html_text()
# add header fields with rvest + httr
url <- "http://spiegel.de/schlagzeilen"
session <- html_session(url, add_headers(From = "my@email.com"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
session <- html_session(url, add_headers(From = "munzert@hertie-school.org"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
?runif
runif(1, 0, 1)
runif(2, 0, 1)
runif(1, 0, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
browseURL("http://www.nytimes.com/robots.txt")
browseURL("https://developer.apple.com/library/content/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/ScheduledJobs.html")
Sys.time()
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
datetime
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
# load RSelenium
library(tidyverse)
library(rvest)
library(RSelenium)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# enter keyword in title field
xpath <- '//*[@id="main"]/div[1]/div[2]/input'
titleElem <- remDr$findElement(using = 'xpath', value = xpath)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
# load RSelenium
library(tidyverse)
library(rvest)
library(RSelenium)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
install.packages("tidytext")
# load RSelenium
library(tidyverse)
library(rvest)
library(RSelenium)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
# initiate Selenium driver
rD <- rsDriver(browser = "firefox")
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
library(httr)
devtools::install_github("ropensci/RSelenium")
install.packages("devtools")
devtools::install_github("ropensci/RSelenium")
library(RSelenium)
library(tidyverse)
library(rvest)
library(RSelenium)
# install current version of Java SE Runtime Environment
browseURL("https://duckduckgo.com/?q=java+download&va=z&t=hk&ia=web")
driver <- remoteDriver()
rD <- remoteDriver()
# set up connection
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.iea.org/policies"
remDr$navigate(url)
library(tidyverse)
library(rvest)
library(RSelenium)
# check currently installed version of Java
system("java -version")
# initiate Selenium driver
rD <- rsDriver()
rD <- rsDriver(version = "3.9.1", chromever = "74.0.3729.6") # chrome is causing some issues on my system, so I switch to firefox
rD <- rsDriver(version = "4.0.0-alpha-1", chromever = "95.0.4638.54") # chrome is causing some issues on my system, so I switch to firefox
rD <- rsDriver(version = "4.0.0-alpha-1", chromever = "96.0.4664.18") # chrome is causing some issues on my system, so I switch to firefox
library(tidyverse)
library(rvest)
library(magrittr)
library(jsonlite)
library(ipapi)
devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
?geolocate
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
View(ip_df)
