url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# enter keyword in title field
xpath <- '//*[@id="main"]/div[1]/div[2]/input'
titleElem <- remDr$findElement(using = 'xpath', value = xpath)
titleElem$sendKeysToElement(list("data")) # enter key word
# select requested title data
xpath <- '//*[@id="main"]/div[8]/div[2]/select/option[7]' # plot
titledatElem1 <- remDr$findElement(using = 'xpath', value = xpath)
titledatElem1$clickElement() # click on list element
xpath <- '//*[@id="main"]/div[8]/div[2]/select/option[10]' # technical info
titledatElem2 <- remDr$findElement(using = 'xpath', value = xpath)
titledatElem2$clickElement() # click on list element
# scroll to end of page (just for fun)
webElem <- remDr$findElement("css", "body")
webElem$sendKeysToElement(list(key = "end"))
# click on search button
xpath <- '//*[@id="main"]/p[3]/button'
searchElem <- remDr$findElement(using = 'xpath', value = xpath)
searchElem$clickElement() # click on button
# store index page
output <- remDr$getPageSource(header = TRUE)
dir.create("data")
output[[1]]
write(output[[1]], file = "data/imdb-data-movies.html")
# close connection
remDr$closeServer()
# parse html
content <- read_html("data/imdb-data-movies.html")
titles <- html_nodes(content, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "lister-item-header", " " ))]//a') %>% html_text
head(titles)
foo <- jsonlite::fromJSON("https://projects.fivethirtyeight.com/2020-election-forecast/house_seats.json")
foo
library(tidyverse)
library(rvest)
library(magrittr)
library(jsonlite)
## overview
browseURL("http://ropensci.org/")
library(ipapi)
devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
library(ipapi)
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
View(ip_df)
# API documentation
browseURL("http://ip-api.com/docs/")
# manual API call, XML data
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::read_xml(url)
ip_list <- as_list(ip_parsed)
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
# manual API call, JSON data
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
as.data.frame(ip_parsed, stringsAsFactors = FALSE)
ip_parsed <- jsonlite::fromJSON(url, flatten = TRUE)
ip_parsed %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
# modify call
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/www.spiegel.de") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
# build function
ipapi_grabber <- function(ip = "") {
dat <- fromJSON(paste0("http://ip-api.com/json/", ip)) %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("193.17.243.1")
#!/usr/local/bin/Rscript
library(stringr)
library(magrittr)
library(httr)
setwd("/Users/simonmunzert/GitHub/ds-workshop-webscraping")
setwd("/Users/s.munzert/GitHub/ds-workshop-webscraping")
setwd("/Users/s.munzert/GitHub/ds-workshop-webscraping/code")
url <- "http://www.spiegel.de/schlagzeilen/"
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
content(url_out, as = "text") %>% write(file = str_c("spiegel-headlines/spiegel-", datetime, ".html"))
#!/usr/local/bin/Rscript
library(stringr)
library(magrittr)
library(httr)
setwd("/Users/s.munzert/GitHubds-workshop-webscraping/code")
setwd("/Users/s.munzert/GitHub/ds-workshop-webscraping/code")
url <- "http://www.spiegel.de/schlagzeilen/"
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
content(url_out, as = "text") %>% write(file = str_c("spiegel-headlines/spiegel-", datetime, ".html"))
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
library(ipapi)
## overview
browseURL("http://ropensci.org/")
browseURL("https://cran.r-project.org/web/views/WebTechnologies.html")
# API documentation
browseURL("http://ip-api.com/")
library(ipapi)
?geolocate
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
# API documentation
browseURL("http://ip-api.com/docs/")
# manual API call, XML data
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::read_xml(url)
ip_parsed
ip_list <- as_list(ip_parsed)
ip_list
ip_list %>% unlist
ip_list %>% unlist %>% t
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
# manual API call, XML data
url <- "http://ip-api.com/xml/72.33.67.89"
ip_parsed <- xml2::read_xml(url)
ip_list <- as_list(ip_parsed)
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
# manual API call, JSON data
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed
as.data.frame(ip_parsed, stringsAsFactors = FALSE)
ip_parsed <- jsonlite::fromJSON(url, flatten = TRUE)
ip_parsed
# modify call
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/www.spiegel.de") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
# build function
ipapi_grabber <- function(ip = "") {
dat <- fromJSON(paste0("http://ip-api.com/json/", ip)) %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("193.17.243.1")
?t
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t
library(rtweet)
install.packages("rtweet")
library(rvest)
library(httr)
?html_read
?read_html
# add User-Agent string
url <- "http://spiegel.de/schlagzeilen"
browseURL("http://www.whoishostingthis.com/tools/user-agent/")
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36"
session <- html_session(url, user_agent(uastring))
# add header fields with rvest + httr
session <- html_session(url, add_headers(From = "my@email.com", `User-Agent` = "R Scraper"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
# add header fields with httr::GET
browseURL("http://httpbin.org")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
GET("http://httpbin.org/headers")
R.Version()$version.string
GET("http://httpbin.org/headers", add_headers(`User-Agent` = "Hello there!"))
GET("http://httpbin.org/headers", add_headers(`User-Agent` = R.Version()$version.string))
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
# example
url_response <- GET("http://spiegel.de/schlagzeilen",
add_headers(From = "my@email.com"))
url_parsed <- url_response  %>% read_html()
url_parsed %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
url_parsed %>% html_nodes(".mr-6") %>%  html_text()
# add header fields with rvest + httr
url <- "http://spiegel.de/schlagzeilen"
session <- html_session(url, add_headers(From = "my@email.com"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
session <- html_session(url, add_headers(From = "munzert@hertie-school.org"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
?runif
runif(1, 0, 1)
runif(2, 0, 1)
runif(1, 0, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
runif(1, 1, 2)
browseURL("http://www.nytimes.com/robots.txt")
browseURL("https://developer.apple.com/library/content/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/ScheduledJobs.html")
Sys.time()
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
datetime
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
# 1. create script (e.g., "spiegel_scraper.R")
# 2. add "#!/usr/local/bin/Rscript" to the top of the script
# 3. create plist file
# 4. load plist file into launchd scheduler and start it (via Terminal):
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
# 5. stop and unload it when desired
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
# load RSelenium
library(tidyverse)
library(rvest)
library(RSelenium)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# enter keyword in title field
xpath <- '//*[@id="main"]/div[1]/div[2]/input'
titleElem <- remDr$findElement(using = 'xpath', value = xpath)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
# load RSelenium
library(tidyverse)
library(rvest)
library(RSelenium)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
install.packages("tidytext")
# load RSelenium
library(tidyverse)
library(rvest)
library(RSelenium)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
# initiate Selenium driver
rD <- rsDriver(browser = "firefox")
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
library(httr)
devtools::install_github("ropensci/RSelenium")
install.packages("devtools")
devtools::install_github("ropensci/RSelenium")
library(RSelenium)
library(tidyverse)
library(rvest)
library(RSelenium)
# install current version of Java SE Runtime Environment
browseURL("https://duckduckgo.com/?q=java+download&va=z&t=hk&ia=web")
driver <- remoteDriver()
rD <- remoteDriver()
# set up connection
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.iea.org/policies"
remDr$navigate(url)
library(tidyverse)
library(rvest)
library(RSelenium)
# check currently installed version of Java
system("java -version")
# initiate Selenium driver
rD <- rsDriver()
rD <- rsDriver(version = "3.9.1", chromever = "74.0.3729.6") # chrome is causing some issues on my system, so I switch to firefox
rD <- rsDriver(version = "4.0.0-alpha-1", chromever = "95.0.4638.54") # chrome is causing some issues on my system, so I switch to firefox
rD <- rsDriver(version = "4.0.0-alpha-1", chromever = "96.0.4664.18") # chrome is causing some issues on my system, so I switch to firefox
library(tidyverse)
library(rvest)
library(magrittr)
library(jsonlite)
library(ipapi)
devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
?geolocate
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
View(ip_df)
## load packages
library(tidyverse)
library(rvest)
library(stringr)
# devtools::install_github("hrbrmstr/nominatim")
library(nominatim)
library(sf)
library(rnaturalearth)
install.packages("nominatim")
browseURL("http://www.biermap24.de/brauereiliste.php")
## step 1: fetch list of cities with breweries
url <- "http://www.biermap24.de/brauereiliste.php"
content <- read_html(url)
anchors <- html_nodes(content, xpath = "//tr/td[3]")
cities <- html_text(anchors)
cities
content <- read_html(url, encoding = "utf-8")
anchors <- html_nodes(content, xpath = "//tr/td[3]")
cities <- html_text(anchors)
cities
cities <- str_trim(cities)
length(cities)
length(unique(cities))
sort(table(cities))
unique_cities <- unique(cities)
## step 2: geocode cities
# get free key for mapquest API at browseURL("https://developer.mapquest.com/")
load("/Users/s.munzert/rkeys.RDa") # import API key (or paste it here in openstreetmap object)
## step 2: geocode cities
# get free key for mapquest API at browseURL("https://developer.mapquest.com/")
load("/Users/simonmunzert/rkeys.RDa") # import API key (or paste it here in openstreetmap object)
n_cities <- length(unique_cities)
coords_df <- data.frame(city = rep(NA, n_cities), lon = rep(NA, n_cities), lat = rep(NA, n_cities), stringsAsFactors = FALSE)
if (!file.exists("coords_breweries.RData")){
for (i in 1:n_cities) {
coords <- try(nominatim::osm_search(unique_cities[i], country_codes = "de", key = openstreetmap))
coords_df$city[i] <- unique_cities[i]
if(nrow(coords) > 0) {
coords_df$lon[i] <- as.numeric(coords$lon)
coords_df$lat[i] <- as.numeric(coords$lat)
}else{
coords_df$lon[i] <- NA
coords_df$lat[i] <- NA
}
}
save(coords_df, file="coords_breweries.RData")
} else {
load("coords_breweries.RData")
}
head(coords_df)
## step 3: plot breweries of Germany
pos <- filter(coords_df, lon >= 6, lon <= 15, lat >= 47, lat <= 55)
worldmap <- rnaturalearth::ne_countries(scale = 'medium', type = 'map_units', returnclass = 'sf')
germany <- worldmap[worldmap$name == 'Germany',]
ggplot() + geom_sf(data = germany) + theme_bw() + geom_point(aes(lon,lat), data = pos, size = .5, color = "red")
pos
head(coords_df)
tabyl(coords_df, city)
tabyl(coords_df$city)
library(janitor)
tabyl(coords_df$city)
tabyl(coords_df$city) %>% head()
tabyl(coords_df$city) %>% arrange(n) %>% head()
sort(table(cities))
tabyl(cities)
tabyl(cities) %>% arrange(n) %>% head()
tabyl(cities) %>% arrange(desc(n)) %>% head()
## load packages -----------------
library(rvest)
# import running example
parsed_doc <- read_html("http://www.r-datacollection.com/materials/ch-4-xpath/fortunes/fortunes.html")
parsed_doc
?html_elements
#remotes::install_github("jhelvy/xaringanBuilder", dependencies = TRUE, force = TRUE)
library(renderthis)
user <- "simonmunzert"
path <- paste0("/Users/", user, "/Munzert Dropbox/Simon Munzert/github/intro-to-data-science-22/lectures/")
session <- "06-webdata"
setwd(paste0(path, session))
to_pdf(paste0(session, ".html"), complex_slides = TRUE, partial_slides = FALSE)
## load packages
library(tidyverse)
library(rvest)
library(stringr)
library(nominatim)
library(sf)
library(rnaturalearth)
library(janitor)
browseURL("http://www.biermap24.de/brauereiliste.php")
## step 1: fetch list of cities with breweries
url <- "http://www.biermap24.de/brauereiliste.php"
content <- read_html(url, encoding = "utf-8")
anchors <- html_nodes(content, xpath = "//tr/td[3]")
anchors
cities <- html_text(anchors)
cities
cities <- str_trim(cities)
length(cities)
length(unique(cities))
tabyl(cities) %>% arrange(desc(n)) %>% head()
setwd("/Users/simonmunzert/Munzert Dropbox/Simon Munzert/github/ds-workshop-webscraping/code/")
n_cities <- length(unique_cities)
coords_df <- data.frame(city = rep(NA, n_cities), lon = rep(NA, n_cities), lat = rep(NA, n_cities), stringsAsFactors = FALSE)
tabyl(cities) %>% arrange(desc(n)) %>% head()
unique_cities <- unique(cities)
## step 2: geocode cities
# get free key for mapquest API at browseURL("https://developer.mapquest.com/")
load("/Users/simonmunzert/rkeys.RDa") # import API key (or paste it here in openstreetmap object)
n_cities <- length(unique_cities)
coords_df <- data.frame(city = rep(NA, n_cities), lon = rep(NA, n_cities), lat = rep(NA, n_cities), stringsAsFactors = FALSE)
if (!file.exists("coords_breweries.RData")){
for (i in 1:n_cities) {
coords <- try(nominatim::osm_search(unique_cities[i], country_codes = "de", key = openstreetmap))
coords_df$city[i] <- unique_cities[i]
if(nrow(coords) > 0) {
coords_df$lon[i] <- as.numeric(coords$lon)
coords_df$lat[i] <- as.numeric(coords$lat)
}else{
coords_df$lon[i] <- NA
coords_df$lat[i] <- NA
}
}
save(coords_df, file="coords_breweries.RData")
} else {
load("coords_breweries.RData")
}
head(coords_df)
## step 3: plot breweries of Germany
pos <- filter(coords_df, lon >= 6, lon <= 15, lat >= 47, lat <= 55)
worldmap <- rnaturalearth::ne_countries(scale = 'medium', type = 'map_units', returnclass = 'sf')
germany <- worldmap[worldmap$name == 'Germany',]
ggplot() + geom_sf(data = germany) + theme_bw() + geom_point(aes(lon,lat), data = pos, size = .5, color = "red")
anchors <- html_nodes(content, xpath = "//tr/td[3]")
anchors
library(stringr)
# example
raw.data <- "555-1239Moe Szyslak(636) 555-0113Burns, C. Montgomery555-6542Rev. Timothy Lovejoy555 8904Ned Flanders636-555-3226Simpson, Homer5553642Dr. Julius Hibbert"
name <- unlist(str_extract_all(raw.data, "[[:alpha:]., ]{2,}"))
name
phone <- unlist(str_extract_all(raw.data, "\\(?(\\d{3})?\\)?(-| )?\\d{3}(-| )?\\d{4}"))
phone
data.frame(name = name, phone = phone)
library(tidyverse)
library(rvest)
library(stringr)
library(nominatim)
library(sf)
library(rnaturalearth)
library(janitor)
browseURL("http://www.biermap24.de/brauereiliste.php")
## step 1: fetch list of cities with breweries
url <- "http://www.biermap24.de/brauereiliste.php"
content <- read_html(url, encoding = "utf-8")
anchors <- html_nodes(content, xpath = "//tr/td[3]")
anchors
cities <- html_text(anchors)
cities
cities <- str_trim(cities)
length(cities)
length(unique(cities))
tabyl(cities) %>% arrange(desc(n)) %>% head()
unique_cities <- unique(cities)
## step 2: geocode cities
# get free key for mapquest API at browseURL("https://developer.mapquest.com/")
load("/Users/simonmunzert/rkeys.RDa") # import API key (or paste it here in openstreetmap object)
n_cities <- length(unique_cities)
coords_df <- data.frame(city = rep(NA, n_cities), lon = rep(NA, n_cities), lat = rep(NA, n_cities), stringsAsFactors = FALSE)
if (!file.exists("coords_breweries.RData")){
for (i in 1:n_cities) {
coords <- try(nominatim::osm_search(unique_cities[i], country_codes = "de", key = openstreetmap))
coords_df$city[i] <- unique_cities[i]
if(nrow(coords) > 0) {
coords_df$lon[i] <- as.numeric(coords$lon)
coords_df$lat[i] <- as.numeric(coords$lat)
}else{
coords_df$lon[i] <- NA
coords_df$lat[i] <- NA
}
}
save(coords_df, file="coords_breweries.RData")
} else {
load("coords_breweries.RData")
}
head(coords_df)
## step 3: plot breweries of Germany
pos <- filter(coords_df, lon >= 6, lon <= 15, lat >= 47, lat <= 55)
worldmap <- rnaturalearth::ne_countries(scale = 'medium', type = 'map_units', returnclass = 'sf')
germany <- worldmap[worldmap$name == 'Germany',]
ggplot() + geom_sf(data = germany) + theme_bw() + geom_point(aes(lon,lat), data = pos, size = .5, color = "red")
vec <- c(1, 2, 3)
vec <- c("apple", "banana", "orange")
vec[2]
anchors <- html_nodes(content, xpath = "//tr/td[3]")
anchors
cities <- html_text(anchors)
cities
raw.data <- "555-1239Moe Szyslak(636) 555-0113Burns, C. Montgomery555-6542Rev. Timothy Lovejoy555 8904Ned Flanders636-555-3226Simpson, Homer5553642Dr. Julius Hibbert"
name <- unlist(str_extract_all(raw.data, "[[:alpha:]., ]{2,}"))
name
phone <- unlist(str_extract_all(raw.data, "\\(?(\\d{3})?\\)?(-| )?\\d{3}(-| )?\\d{4}"))
phone
# running example
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
# pipe operator
str_extract(example.obj, "tiny|sentence")
# pipe operator
str_extract_all(example.obj, "tiny|sentence")
unlist(str_extract_all(example.obj, "tiny|sentence"))
